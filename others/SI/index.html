<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Call for Papers: IEEE TPAMI Special Issue</title>

    <!-- Meta tags for search engines to crawl -->
    <meta name="robots" content="index,follow">
    <meta name="description"
          content="Call for Papers: IJCV Special Issue"
          .
    >
    <meta name="keywords" content="Call for Papers: IJCV Special Issue">
    <link rel="author" href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">

    <!-- Fonts and stuff -->
    <link href="./css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
    <link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
    <script async="" src="./prettify.js"></script>

</head>

<body>
<div id="content">
    <div id="content-inner">

        <center><img src="./img/IJCV_LOGO-1.png" border="0" width="99.5%"><br>
            <!--            Left: The specification of the proposed 100-Driver for distracted driver classification. Right: The comparisons of different datasets.-->
        </center>

        <div class="section head">
            <h1> Call for Papers: IJCV Special Issue</h1>
            <h1> Open-World Visual Recognition</h1>

            <!--            <div class="authors">-->
            <!--                <a >Jing Wang</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Wenjing Li</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Fang Li</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Jun Zhang</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Zhongcheng Wu</a><sup>1,2</sup>&nbsp;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Enver Sangineto</a><sup>1</sup>&nbsp;&ndash;&gt;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Stéphane Lathuilière</a><sup>2</sup>&nbsp;&ndash;&gt;-->
            <!--                <a href="https://zhunzhong.site/">Zhun Zhong</a><sup>3</sup>&nbsp;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Moin Nabi</a><sup>3</sup>&nbsp;&ndash;&gt;-->
            <!--                <a >Nicu Sebe</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-->
            <!--            </div>-->

            <!--            <div class="affiliations">-->
            <!--                1. Chinese Academy of Sciences, China <br>-->
            <!--                2. University of Science and Technology of China, China <br>-->
            <!--                3. University of Trento, Italy-->
            <!--            </div>-->

            <!--            <div class="affiliations">-->
            <!--                wjli007@mail.ustc.edu.cn-->
            <!--            </div>-->
        </div>


        <div class="section abstract">
            <h2>Introduction</h2>
            <br>
            <p>Visual recognition is a critical task in computer vision, which has gained significant attention in recent years due to its numerous applications in various
                fields, including image classification, object detection, semantic segmentation,
                and instance retrieval for autonomous driving and intelligent surveillance. Over
                the past decade, deep learning techniques and large-scale datasets have contributed to remarkable advancements in the performance of visual recognition
                systems. However, existing visual models are often limited by their closed-world
                assumptions, where all possible classes and domains are known in advance, and
                all data is given at once. Such assumptions are not applicable in practical scenarios, where novel or previously unseen classes and domains can arise, and
                data may be continually coming or decentralized due to data privacy concerns.
                One example of such a scenario is an autonomous vehicle encounters new traffic
                patterns, a medical AI system detects new diseases, or an intelligent system encounters a criminal wearing a new type of disguise or clothing. Another example
                is when a healthcare organization collects patient data from multiple hospitals,
                and the data may be decentralized and continually coming in due to privacy
                concerns. Moreover, the challenge of multi-modality poses another obstacle, as
                visual recognition systems must be capable of integrating and handling data
                from multiple sources, such as images, videos, texts, and 3D models. Therefore,
                new methods and techniques are needed to address the challenges of open-world
                visual recognition and enable visual recognition systems to perform effectively
                in practical scenarios.</p>
        </div>

        <div class="section abstract">
            <h2>Aims&Scope</h2>
            <br>
            <p>This special issue invites innovative research papers that aim to address these
                challenges and propose novel techniques for open-world visual recognition. Potential topics of interest include, but are not limited to:</p>
            <li><b>Novel Class Discovery</b>, where the goal is to discover underlying semantic clusters for unlabeled data including unseen classes</li>
            <li><b>Open-Set Semi-Supervised Learning</b>, where the system should learn from both labeled and unlabeled data, and distinguish between known and unknown classes during testing</li>
            <li><b>Open Vocabulary Visual Learning</b>, where the goal is to train a model that can recognize a broader range of visual concepts, including new and rare ones, using a limited pre-defined set of categories</li>
            <li><b>Out-of-Distribution Detection</b>, where the goal is to  distinguish between in-distribution and out-of-distribution samples</li>
            <li><b>Robust/Adversarial Learning</b>, where the goal is to improve the model's robustness to distributional shifts, such as adversarial perturbations and corruptions</li>
            <li><b>Open-World Domain Adaptation</b>, where the goal is to adapt/generalize the model to target domains under open-world scenarios, such as universal, source-free, test-time domain adaptation, and domain generalization</li>
            <p></p>
            <p>Moreover, this special issue also welcomes papers that focus on:</p>
            <li><b>Developing new techniques for continual learning, federated learning and multi-modality learning</b> in the context of open-world visual recognition</li>
            <li><b>Building datasets and benchmarks</b> for facilitating the study of open-world visual recognition</li>
            <p></p>
            We encourage submissions that cover a broad range of visual recognition tasks, including but not limited to image classification, object detection, semantic segmentation, action recognition and pose estimation.
        </div>


        <div class="section abstract">
            <h2>Submission Guidelines</h2>
            <br>
            <p>The submission system will be available soon. Authors should select the "S.I.: Open-World Visual Recognition" article type when submitting their manuscripts to the journal.</p>
            <p>Submitted papers should present original, unpublished work, relevant to one of the topics of the Special Issue. All submitted papers will be evaluated on the basis of relevance, significance of contribution, technical quality, scholarship, and quality of presentation, by at least three independent reviewers. It is the policy of the journal that no submission, or substantially overlapping submission, be published or be under review at another journal or conference at any time during the review process. Papers should be prepared according to the journal's guidelines.</p>
        </div>

        <div class="section abstract">
            <h2>Important Dates</h2>
            <br>
            <li> Submission deadline: October 15, 2023</li>
            <li> First review notification: December 15, 2023</li>
            <li> Revised submission deadline: February 15, 2024</li>
            <li> Final review notification: March 15, 2024</li>
            <li> Final manuscript due: April 15, 2024</li>
            <li>Publication date: early</li>
        </div>


        <div class="section abstract">
            <h2>Guest Editors</h2>
            <br>
            <li>  <a href="https://zhunzhong.site">Zhun Zhong</a>: Assistant Professor, University of Trento, Italy, <a href = "mailto: zhunzhong007@gmail.com">zhunzhong007@gmail.com</a></li>

            <li>  <a href="https://lynnhongliu.github.io/hliu">Hong Liu</a>, Fellowship Researcher, National Institute of Informatics, Japan, <a href = "mailto: hliu@nii.ac.jp">hliu@nii.ac.jp</a></li>

            <li> <a href="https://ycui.me">Yin Cui</a>, Senior Research Scientist, Google, USA, <a href = "mailto:  yincui@google.com"> yincui@google.com</a></li>

            <li>  <a href="http://www.satoh-lab.nii.ac.jp">Shin'ichi Satoh</a>: Professor, National Institute of Informatics, Japan, <a href = "mailto: satoh@nii.ac.jp">satoh@nii.ac.jp</a></li>

            <li>  <a href="https://disi.unitn.it/~sebe">Nicu Sebe</a>: Professor, University of Trento, Italy, <a href = "mailto: niculae.sebe@unitn.it">niculae.sebe@unitn.it</a></li>

            <li>  <a href="http://faculty.ucmerced.edu/mhyang">Ming-Hsuan Yang</a> : Professor, University of California at Merced, USA, <a href = "mailto: mhyang@ucmerced.edu">mhyang@ucmerced.edu</a></li>

        </div>

        <div class="section method">
            <h2>Concact</h2>
            <br>
            If you have any question related to this IJCV SI, please contact <a href="https://zhunzhong.site">Zhun Zhong</a> or <a href="https://lynnhongliu.github.io/hliu">Hong Liu</a>.
        </div>

</body>
</html>
